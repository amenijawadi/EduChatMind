"""
Convertit votre mod√®le XLM-RoBERTa entra√Æn√© au format compatible Rasa
IMPORTANT: Placez votre fichier model.safetensors dans ./kaggle_model/
"""
import torch
import json
import os
from transformers import XLMRobertaForSequenceClassification, XLMRobertaConfig

# Configuration
KAGGLE_MODEL_PATH = "./kaggle_model"  # Dossier contenant model.safetensors
OUTPUT_PATH = "./models"
NUM_LABELS = 28

# Les 28 √©motions de votre mod√®le (ordre CRUCIAL - doit correspondre √† l'entra√Ænement)
EMOTION_LABELS = [
    'admiration', 'amusement', 'anger', 'annoyance', 'approval',
    'caring', 'confusion', 'curiosity', 'desire', 'disappointment',
    'disapproval', 'disgust', 'embarrassment', 'excitement', 'fear',
    'gratitude', 'grief', 'joy', 'love', 'nervousness',
    'neutral', 'optimism', 'pride', 'realization', 'relief',
    'remorse', 'sadness', 'surprise'
]

def convert_model():
    """Convertit le mod√®le Kaggle au format utilisable par Rasa"""
    
    print("="*80)
    print("üîÑ CONVERSION DU MOD√àLE XLM-RoBERTa (28 √âMOTIONS)")
    print("="*80)
    
    # V√©rifier que le mod√®le source existe
    safetensors_path = os.path.join(KAGGLE_MODEL_PATH, "model.safetensors")
    if not os.path.exists(safetensors_path):
        print(f"‚ùå ERREUR: Fichier non trouv√©: {safetensors_path}")
        print(f"   Veuillez placer model.safetensors de Kaggle dans {KAGGLE_MODEL_PATH}/")
        return False
    
    print(f"‚úÖ Mod√®le source trouv√©: {safetensors_path}")
    
    # Cr√©er le dossier de sortie
    os.makedirs(OUTPUT_PATH, exist_ok=True)
    
    # Charger le mod√®le depuis safetensors
    print("\nüì• Chargement du mod√®le...")
    try:
        model = XLMRobertaForSequenceClassification.from_pretrained(
            KAGGLE_MODEL_PATH,
            num_labels=NUM_LABELS,
            problem_type="single_label_classification"
        )
        print("‚úÖ Mod√®le charg√© avec succ√®s!")
    except Exception as e:
        print(f"‚ùå Erreur lors du chargement: {e}")
        return False
    
    # Sauvegarder au format PyTorch (.pt)
    print("\nüíæ Conversion au format PyTorch (.pt)...")
    model_dict = {
        'model_state_dict': model.state_dict(),
        'num_labels': NUM_LABELS,
        'model_type': 'xlm-roberta-base',
        'emotion_labels': EMOTION_LABELS
    }
    
    torch.save(model_dict, os.path.join(OUTPUT_PATH, "model.pt"))
    print("‚úÖ model.pt cr√©√©!")
    
    # Cr√©er metadata.json
    print("\nüìù Cr√©ation de metadata.json...")
    metadata = {
        "model_name": "xlm-roberta-base",
        "num_labels": NUM_LABELS,
        "emotion_labels": EMOTION_LABELS,
        "threshold": 0.5,  # Ajuster si n√©cessaire
        "max_length": 64,
        "problem_type": "single_label_classification",
        "training_info": {
            "best_epoch": 17,
            "val_f1_macro": 0.7956,
            "test_f1_macro": 0.7953
        }
    }
    
    with open(os.path.join(OUTPUT_PATH, "metadata.json"), 'w') as f:
        json.dump(metadata, f, indent=2)
    print("‚úÖ metadata.json cr√©√©!")
    
    # R√©sum√©
    print("\n" + "="*80)
    print("‚úÖ CONVERSION R√âUSSIE!")
    print("="*80)
    print(f"\nüìÅ Fichiers cr√©√©s dans {OUTPUT_PATH}/:")
    for f in os.listdir(OUTPUT_PATH):
        file_path = os.path.join(OUTPUT_PATH, f)
        if os.path.isfile(file_path):
            size = os.path.getsize(file_path)
            print(f"  ‚úì {f} ({size:,} bytes)")
    
    print(f"\nüéØ √âmotions support√©es: {len(EMOTION_LABELS)}")
    print(f"üìä Meilleur F1 (test): 0.7953")
    print(f"\n‚ö†Ô∏è  PROCHAINE √âTAPE: Ex√©cutez tokenizer.py pour t√©l√©charger le tokenizer!")
    
    return True

if __name__ == "__main__":
    success = convert_model()
    if success:
        print("\nüéâ Pr√™t √† √™tre utilis√© avec Rasa!")
    else:
        print("\n‚ùå La conversion a √©chou√©. V√©rifiez les erreurs ci-dessus.")